{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "218ba43b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dealmonitor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# from .features import extract_price_features  # use your features.py\u001b[39;00m\n\u001b[32m     23\u001b[39m sys.path.append(os.path.abspath(\u001b[33m\"\u001b[39m\u001b[33mdealmonitor/backend/src\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdealmonitor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeatures\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeatures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m extract_price_features\n\u001b[32m     26\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     28\u001b[39m dotenv.load_dotenv()  \u001b[38;5;66;03m# Load environment variables from .env file\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'dealmonitor'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import logging\n",
    "import subprocess\n",
    "import dotenv\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "# from imbalance-learn import \n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "import dagshub\n",
    "\n",
    "# from .features import extract_price_features  # use your features.py\n",
    "\n",
    "sys.path.append(os.path.abspath(\"dealmonitor/backend/src\"))\n",
    "from dealmonitor.features.features import extract_price_features\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "dotenv.load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "MODEL_LATEST = \"models/model_latest.pkl\"\n",
    "MODEL_BEST = \"models/model_best.pkl\"\n",
    "\n",
    "# 1️⃣ MLflow / DagsHub Setup (hier kannst du auch dotenv nehmen)\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = os.environ[\"DAGSHUB_REPO_URL\"]\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = os.environ[\"DAGSHUB_USERNAME\"]\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = os.environ[\"DAGSHUB_TOKEN\"]\n",
    "dagshub.init(repo_owner=os.environ[\"MLFLOW_TRACKING_USERNAME\"], repo_name='dealmonitor_ml', mlflow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc600f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "        from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "        models = {\n",
    "            # \"mlp\": MLPClassifier(),\n",
    "        }\n",
    "\n",
    "        param_grids = {\n",
    "            \"mlp\": {\n",
    "                \"hidden_layer_sizes\": [(32,)], #(64,), (64, 32), (128, 64), (256, 128, 64)],\n",
    "                'activation': ['relu'], #'tanh', 'logistic'],\n",
    "                'solver': ['adam'], #'sgd'],\n",
    "                'alpha': [0.0001], #0.001],\n",
    "                'learning_rate_init': [0.001], #0.01],\n",
    "                \"max_iter\": [5000],\n",
    "                \"random_state\": [42],\n",
    "            },\n",
    "        }\n",
    "\n",
    "        results = {}\n",
    "        for name, model in models.items():\n",
    "            gs = GridSearchCV(model, param_grids[name], scoring=\"f1\", cv=3, n_jobs=-1)\n",
    "            gs.fit(X_train, y_train)\n",
    "            results[name] = {\n",
    "                \"best_score\": gs.best_score_,\n",
    "                \"best_params\": gs.best_params_,\n",
    "                \"best_estimator\": gs.best_estimator_\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c31a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "        from xgboost import XGBClassifier\n",
    "\n",
    "        models = {\n",
    "            \"xgb\": XGBClassifier(),\n",
    "        }\n",
    "\n",
    "        param_grids = {\n",
    "            \"xgb\": {\n",
    "                \"n_estimators\": [100],# 250, 500, 1000],\n",
    "                \"learning_rate\": [0.001],# 0.01, 0.1],\n",
    "                \"max_depth\": [3],# 5, 7, 10],\n",
    "                \"subsample\": [0.6],# 0.8, 1.0],\n",
    "                \"colsample_bytree\": [0.6],# 0.8, 1.0],\n",
    "                \"random_state\": [42]\n",
    "            },\n",
    "        }\n",
    "\n",
    "        results = {}\n",
    "        for name, model in models.items():\n",
    "            gs = GridSearchCV(model, param_grids[name], scoring=\"f1\", cv=3, n_jobs=-1)\n",
    "            gs.fit(X_train, y_train)\n",
    "            results[name] = {\n",
    "                \"best_score\": gs.best_score_,\n",
    "                \"best_params\": gs.best_params_,\n",
    "                \"best_estimator\": gs.best_estimator_\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08bb555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "        # Models\n",
    "        from sklearn.neural_network import MLPClassifier\n",
    "        from xgboost import XGBClassifier\n",
    "        from lightgbm import LGBMClassifier\n",
    "\n",
    "        models = {\n",
    "            # \"mlp\": MLPClassifier(),\n",
    "            \"xgb\": XGBClassifier(),\n",
    "            # \"lgbm\": LGBMClassifier()\n",
    "        }\n",
    "\n",
    "        param_grids = {\n",
    "            \"mlp\": {\n",
    "                \"hidden_layer_sizes\": [(32,)], #(64,), (64, 32), (128, 64), (256, 128, 64)],\n",
    "                'activation': ['relu'], #'tanh', 'logistic'],\n",
    "                'solver': ['adam'], #'sgd'],\n",
    "                'alpha': [0.0001], #0.001],\n",
    "                'learning_rate_init': [0.001], #0.01],\n",
    "                \"max_iter\": [5000],\n",
    "                \"random_state\": [42],\n",
    "            },\n",
    "            # \"xgb\": {\n",
    "            #     \"n_estimators\": [100],# 250, 500, 1000],\n",
    "            #     \"learning_rate\": [0.001],# 0.01, 0.1],\n",
    "            #     \"max_depth\": [3],# 5, 7, 10],\n",
    "            #     \"subsample\": [0.6],# 0.8, 1.0],\n",
    "            #     \"colsample_bytree\": [0.6],# 0.8, 1.0],\n",
    "            #     \"random_state\": [42]\n",
    "            # },\n",
    "            # \"lgbm\": {\n",
    "            #     \"n_estimators\": [250],# 500],\n",
    "            #     \"learning_rate\": [0.001],# 0.01, 0.05, 0.1],\n",
    "            #     \"max_depth\": [3],# 5, 7, 10]\n",
    "            # }\n",
    "        }\n",
    "\n",
    "        results = {}\n",
    "        for name, model in models.items():\n",
    "            gs = GridSearchCV(model, param_grids[name], scoring=\"f1\", cv=3, n_jobs=-1)\n",
    "            gs.fit(X_train, y_train)\n",
    "            results[name] = {\n",
    "                \"best_score\": gs.best_score_,\n",
    "                \"best_params\": gs.best_params_,\n",
    "                \"best_estimator\": gs.best_estimator_\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a36fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_df(df=pd.DataFrame()) -> pd.DataFrame:\n",
    "    features = df.apply(lambda row: extract_price_features(row.to_dict()), axis=1)\n",
    "    features_df = pd.DataFrame(list(features))\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def train_nn_model(\n",
    "    data_path: str = \"data/knn_training_set.parquet\",\n",
    "    model_path: str = \"models/nn_model.pkl\"\n",
    "):\n",
    "    with mlflow.start_run(run_name=\"Train NN Model\"):\n",
    "        # Track Input-Data as Artefact\n",
    "        mlflow.log_artifact(data_path)\n",
    "\n",
    "        df = pd.read_parquet(data_path)\n",
    "        logger.info(df.info())\n",
    "\n",
    "        # Feature Engineering\n",
    "        X_full = build_feature_df(df)\n",
    "        X_full = X_full.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        df = df.loc[X_full.index].reset_index(drop=True)\n",
    "        X_full = X_full.reset_index(drop=True)\n",
    "        y = df[\"match_with_user\"].astype(int)\n",
    "\n",
    "        # # One-Hot-Encoding domain feature\n",
    "        # if \"domain\" in X_full.columns:\n",
    "        #     domain_dummies = pd.get_dummies(X_full[\"domain\"], prefix=\"domain\")\n",
    "        #     X_full = X_full.drop(columns=[\"domain\"])\n",
    "        #     X_full = pd.concat([X_full, domain_dummies], axis=1)\n",
    "\n",
    "        meta_cols = [\"raw_data_id\", \"price_user\", \"value_clean\"]\n",
    "        X_full[\"raw_data_id\"] = df[\"raw_data_id\"].values\n",
    "        X_full[\"price_user\"] = df[\"price_user\"].values\n",
    "        X_full[\"value_clean\"] = df[\"value_clean\"].values\n",
    "        meta_full = X_full[meta_cols]\n",
    "        X_features = X_full.drop(columns=meta_cols)\n",
    "\n",
    "        # Oversampling\n",
    "        ros = RandomOverSampler(random_state=42)\n",
    "        X_resampled, y_resampled = ros.fit_resample(X_features, y)\n",
    "        meta_resampled = meta_full.iloc[ros.sample_indices_].reset_index(drop=True)\n",
    "        X_resampled = X_resampled.reset_index(drop=True)\n",
    "        y_resampled = y_resampled.reset_index(drop=True)\n",
    "\n",
    "        # Train/Test Split\n",
    "        unique_ids = meta_resampled[\"raw_data_id\"].unique()\n",
    "        train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=42)\n",
    "        train_mask = meta_resampled[\"raw_data_id\"].isin(train_ids)\n",
    "        test_mask = meta_resampled[\"raw_data_id\"].isin(test_ids)\n",
    "        X_train, y_train = X_resampled[train_mask], y_resampled[train_mask]\n",
    "        X_test, y_test = X_resampled[test_mask], y_resampled[test_mask]\n",
    "        meta_test = meta_resampled[test_mask].copy()\n",
    "\n",
    "        # Modell & Params als Param loggen\n",
    "        # model = HistGradientBoostingClassifier(\n",
    "        #     max_iter=5000,\n",
    "        #     early_stopping=True,\n",
    "        #     random_state=42\n",
    "        # )\n",
    "\n",
    "# Try with many models and many options\n",
    "\n",
    "        # Models\n",
    "        from sklearn.neural_network import MLPClassifier\n",
    "        from xgboost import XGBClassifier\n",
    "        from lightgbm import LGBMClassifier\n",
    "\n",
    "        models = {\n",
    "            # \"mlp\": MLPClassifier(),\n",
    "            \"xgb\": XGBClassifier(),\n",
    "            # \"lgbm\": LGBMClassifier()\n",
    "        }\n",
    "\n",
    "        param_grids = {\n",
    "            \"mlp\": {\n",
    "                \"hidden_layer_sizes\": [(32,)], #(64,), (64, 32), (128, 64), (256, 128, 64)],\n",
    "                'activation': ['relu'], #'tanh', 'logistic'],\n",
    "                'solver': ['adam'], #'sgd'],\n",
    "                'alpha': [0.0001], #0.001],\n",
    "                'learning_rate_init': [0.001], #0.01],\n",
    "                \"max_iter\": [5000],\n",
    "                \"random_state\": [42],\n",
    "            },\n",
    "            # \"xgb\": {\n",
    "            #     \"n_estimators\": [100],# 250, 500, 1000],\n",
    "            #     \"learning_rate\": [0.001],# 0.01, 0.1],\n",
    "            #     \"max_depth\": [3],# 5, 7, 10],\n",
    "            #     \"subsample\": [0.6],# 0.8, 1.0],\n",
    "            #     \"colsample_bytree\": [0.6],# 0.8, 1.0],\n",
    "            #     \"random_state\": [42]\n",
    "            # },\n",
    "            # \"lgbm\": {\n",
    "            #     \"n_estimators\": [250],# 500],\n",
    "            #     \"learning_rate\": [0.001],# 0.01, 0.05, 0.1],\n",
    "            #     \"max_depth\": [3],# 5, 7, 10]\n",
    "            # }\n",
    "        }\n",
    "\n",
    "        results = {}\n",
    "        for name, model in models.items():\n",
    "            gs = GridSearchCV(model, param_grids[name], scoring=\"f1\", cv=3, n_jobs=-1)\n",
    "            gs.fit(X_train, y_train)\n",
    "            results[name] = {\n",
    "                \"best_score\": gs.best_score_,\n",
    "                \"best_params\": gs.best_params_,\n",
    "                \"best_estimator\": gs.best_estimator_\n",
    "            }\n",
    "\n",
    "# Try with more options to one model\n",
    "\n",
    "        # # Definiere das Basis-Modell\n",
    "        # model = MLPClassifier(random_state=42, max_iter=10000)\n",
    "\n",
    "        # # Definiere den Suchraum für Hyperparameter\n",
    "        # param_grid = {\n",
    "        #     'hidden_layer_sizes': [(64,), (128, 64), (256, 128, 64)],\n",
    "        #     'activation': ['relu', 'tanh', 'logistic'],\n",
    "        #     'solver': ['adam', 'sgd'],\n",
    "        #     'alpha': [0.0001, 0.001],\n",
    "        #     'learning_rate_init': [0.001, 0.01]\n",
    "        # }\n",
    "\n",
    "        # # Initialisiere Grid Search mit Cross-Validation (z.B. 3-fach CV)\n",
    "        # grid_search = GridSearchCV(\n",
    "        #     estimator=model,\n",
    "        #     param_grid=param_grid,\n",
    "        #     scoring='f1',\n",
    "        #     n_jobs=-1,\n",
    "        #     cv=3,\n",
    "        #     verbose=2\n",
    "        # )\n",
    "\n",
    "        # # X_train, y_train sind deine Trainingsdaten (bereits vorbereitet)\n",
    "        # grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # print(\"Best parameters set found on development set:\")\n",
    "        # print(grid_search.best_params_)\n",
    "\n",
    "        # print(\"Best CV score:\")\n",
    "        # print(grid_search.best_score_)\n",
    "\n",
    "        # best_model = grid_search.best_estimator_\n",
    "\n",
    "# Models: NN, XGBoost, LightGBM, CatBoost, RandomForest, ExtraTrees, AdaBoost, HistGradientBoosting\n",
    "        \n",
    "        # model = MLPClassifier(\n",
    "        #     hidden_layer_sizes=(256, 128, 64, 32),\n",
    "        #     activation=\"tanh\",\n",
    "        #     solver=\"adam\",\n",
    "        #     max_iter=10000,\n",
    "        #     random_state=42,\n",
    "        #     learning_rate_init=0.001,\n",
    "        # )\n",
    "\n",
    "        # from xgboost import XGBClassifier\n",
    "        # model = XGBClassifier(\n",
    "        #     n_estimators=1000,\n",
    "        #     learning_rate=0.01,\n",
    "        #     max_depth=6,\n",
    "        #     subsample=0.8,\n",
    "        #     colsample_bytree=0.8,\n",
    "        #     reg_alpha=0.1,\n",
    "        #     reg_lambda=1,\n",
    "        #     random_state=42,\n",
    "        #     # early_stopping_rounds=50\n",
    "        # )\n",
    "\n",
    "\n",
    "        # from xgboost import XGBClassifier\n",
    "        # model = XGBClassifier(n_estimators=500, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "        # from lightgbm import LGBMClassifier\n",
    "        # model = LGBMClassifier(n_estimators=500, random_state=42)\n",
    "\n",
    "        # from catboost import CatBoostClassifier\n",
    "        # model = CatBoostClassifier(iterations=500, random_state=42, verbose=0)\n",
    "\n",
    "        # from sklearn.ensemble import RandomForestClassifier\n",
    "        # model = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "\n",
    "        # from sklearn.ensemble import ExtraTreesClassifier\n",
    "        # model = ExtraTreesClassifier(n_estimators=500, random_state=42)\n",
    "\n",
    "        # from sklearn.ensemble import AdaBoostClassifier\n",
    "        # model = AdaBoostClassifier(n_estimators=500, random_state=42)\n",
    "\n",
    "        mlflow.log_params(model.get_params())\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        print(f\"✅ Accuracy: {acc:.3f}\")\n",
    "        print(f\"✅ F1 Score: {f1:.3f}\")\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "        # signature and input example for MLflow\n",
    "        input_example = X_train.iloc[:5].copy()  # oder nimm eine Zeile, je nach Bedarf\n",
    "        signature = infer_signature(input_example, model.predict(input_example))\n",
    "\n",
    "        # Top-K Accuracy\n",
    "        probs = model.predict_proba(X_test)[:, 1]\n",
    "        meta_test = meta_test.reset_index(drop=True)\n",
    "        meta_test[\"proba\"] = probs\n",
    "        top1_correct = 0\n",
    "        top3_correct = 0\n",
    "        total = 0\n",
    "        for raw_data_id, group in meta_test.groupby(\"raw_data_id\"):\n",
    "            sorted_group = group.sort_values(\"proba\", ascending=False)\n",
    "            expected_price = sorted_group[\"price_user\"].iloc[0]\n",
    "            top1_value = sorted_group[\"value_clean\"].iloc[0]\n",
    "            if np.isclose(top1_value, expected_price, atol=0.01):\n",
    "                top1_correct += 1\n",
    "            top3_values = sorted_group[\"value_clean\"].iloc[:3].values\n",
    "            if np.any(np.isclose(top3_values, expected_price, atol=0.01)):\n",
    "                top3_correct += 1\n",
    "            total += 1\n",
    "        top1_acc = top1_correct / total\n",
    "        top3_acc = top3_correct / total\n",
    "        mlflow.log_metric(\"top1_accuracy\", top1_acc)\n",
    "        mlflow.log_metric(\"top3_accuracy\", top3_acc)\n",
    "        print(f\"🎯 Top-1 Accuracy: {top1_acc:.3f} ({top1_correct} of {total})\")\n",
    "        print(f\"🎯 Top-3 Accuracy: {top3_acc:.3f} ({top3_correct} of {total})\")\n",
    "\n",
    "        # Modell speichern & als Artefakt loggen\n",
    "        joblib.dump(model, model_path)\n",
    "        mlflow.set_tag(\"model_version\", \"latest\")\n",
    "        mlflow.set_tag(\"model_path\", \"models/model_latest.pkl\")\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=model,\n",
    "            registered_model_name=\"DealMonitorNN\",\n",
    "            artifact_path=\"sklearn-model\",\n",
    "            signature=signature,\n",
    "            input_example=input_example\n",
    "        )\n",
    "        mlflow.log_artifact(model_path)\n",
    "        print(f\"✅ Model saved at {model_path}\")\n",
    "\n",
    "        # DVC-Bestmodell verwalten wie gehabt\n",
    "        best_model_path = os.path.join(os.path.dirname(model_path), \"model_best.pkl\")\n",
    "        update_best = True\n",
    "        if os.path.exists(best_model_path):\n",
    "            try:\n",
    "                best_model = joblib.load(best_model_path)\n",
    "                y_pred_best = best_model.predict(X_test)\n",
    "                f1_best = f1_score(y_test, y_pred_best)\n",
    "                if f1 <= f1_best:\n",
    "                    update_best = False\n",
    "                    print(f\"ℹ️ Best model kept (f1 {f1_best:.3f} ≥ {f1:.3f})\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Could not evaluate existing best model: {e}\")\n",
    "\n",
    "        if update_best:\n",
    "            joblib.dump(model, best_model_path)\n",
    "            print(f\"🏆 New best model saved at {best_model_path}\")\n",
    "            subprocess.run([\"dvc\", \"add\", \"models/model_best.pkl\"])\n",
    "            subprocess.run([\"git\", \"add\", \"models/model_best.pkl.dvc\", \".gitignore\"])\n",
    "            commit = 'Add/update best and latest model'\n",
    "        else:\n",
    "            commit = 'Add/update latest model'\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        versioned_model_path = model_path.replace(\".pkl\", f\"_{timestamp}.pkl\")\n",
    "        shutil.copy(model_path, versioned_model_path)\n",
    "        symlink_path = os.path.join(os.path.dirname(model_path), \"model_latest.pkl\")\n",
    "        try:\n",
    "            if os.path.exists(symlink_path):\n",
    "                os.remove(symlink_path)\n",
    "            os.symlink(os.path.abspath(model_path), symlink_path)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not update symlink: {e}\")\n",
    "        subprocess.run([\"dvc\", \"add\", \"models/model_latest.pkl\"])\n",
    "        subprocess.run([\"git\", \"add\", \"models/model_latest.pkl.dvc\", \".gitignore\"])\n",
    "        subprocess.run([\"git\", \"commit\", \"-m\", commit])\n",
    "        subprocess.run([\"dvc\", \"push\"])\n",
    "\n",
    "        print(f\"✅ Versioned model saved as {versioned_model_path}\")\n",
    "        print(f\"🔗 Symlink updated: {symlink_path}\")\n",
    "\n",
    "        # Training log\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"model_type\": type(model).__name__,\n",
    "            \"params\": str(model.get_params()),\n",
    "            \"accuracy\": round(acc, 3),\n",
    "            \"f1_score\": round(f1, 3),\n",
    "            \"top1_acc\": round(top1_acc, 3),\n",
    "            \"top3_acc\": round(top3_acc, 3),\n",
    "            \"raw_data_count\": df[\"raw_data_id\"].nunique(),\n",
    "            \"model_path\": versioned_model_path\n",
    "        }\n",
    "        log_path = os.path.join(os.path.dirname(model_path), \"training_log.csv\")\n",
    "        log_df = pd.DataFrame([log_entry])\n",
    "        if os.path.exists(log_path):\n",
    "            log_df.to_csv(log_path, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            log_df.to_csv(log_path, mode='w', header=True, index=False)\n",
    "        print(f\"📝 Training log updated: {log_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7d35d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_nn_model(data_path=\"data/knn_training_set.parquet\", model_path=\"models/nn_model.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dealmonitor_ml_clean_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
